---
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  annotations:
    pipelines.kubeflow.org/run_name: my-0322142832
    tekton.dev/artifact_items: >-
      {"run-a-file": [["mlpipeline-metrics", "/tmp/mlpipeline-metrics.json"],
      ["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"]],
      "run-a-file-2": [["mlpipeline-metrics", "/tmp/mlpipeline-metrics.json"],
      ["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"]]}
    tekton.dev/artifact_bucket: thy-workshop-pipelines
    tekton.dev/template: ''
    sidecar.istio.io/inject: 'false'
    tekton.dev/artifact_endpoint_scheme: 'http://'
    tekton.dev/output_artifacts: >-
      {"run-a-file": [{"key":
      "artifacts/$PIPELINERUN/run-a-file/mlpipeline-metrics.tgz", "name":
      "mlpipeline-metrics", "path": "/tmp/mlpipeline-metrics.json"}, {"key":
      "artifacts/$PIPELINERUN/run-a-file/mlpipeline-ui-metadata.tgz", "name":
      "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"}],
      "run-a-file-2": [{"key":
      "artifacts/$PIPELINERUN/run-a-file-2/mlpipeline-metrics.tgz", "name":
      "mlpipeline-metrics", "path": "/tmp/mlpipeline-metrics.json"}, {"key":
      "artifacts/$PIPELINERUN/run-a-file-2/mlpipeline-ui-metadata.tgz", "name":
      "mlpipeline-ui-metadata", "path": "/tmp/mlpipeline-ui-metadata.json"}]}
    tekton.dev/artifact_endpoint: 'http://minio-service.cansutest.svc:9000'
    pipelines.kubeflow.org/pipeline_spec: '{"name": "my"}'
    pipelines.kubeflow.org/big_data_passing_format: >-
      $(workspaces.$TASK_NAME.path)/artifacts/my-6f8f8/$TASKRUN_NAME/$TASK_PARAM_NAME
    tekton.dev/input_artifacts: '{}'
  resourceVersion: '1106389'
  name: my-6f8f8
  namespace: cansutest
  finalizers:
    - chains.tekton.dev/pipelinerun
  labels:
    custom.tekton.dev/originalPipelineRun: my-6f8f8
    pipeline/runid: 6f8f8280-b1ec-48a5-8806-13c6e1a26c4f
    pipelines.kubeflow.org/generation: ''
    pipelines.kubeflow.org/pipelinename: ''
    tekton.dev/pipeline: my-6f8f8
spec:
  pipelineSpec:
    tasks: # limit ve resource bilgisi bu listedeki her bir task'a eklenmeli
      - name: run-a-file
        taskSpec:
          metadata:
            annotations:
              elyra/node-file-name: hello.ipynb
              elyra/pipeline-source: my.pipeline
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Run a file", "outputs": [], "version": "Run a
                file@sha256=0524eb926f2a2f2b0ba7b6b5f774abadf3e9f38f59677bf19f314fe5a8a74faa"}
              pipelines.kubeflow.org/task_display_name: hello
            labels:
              elyra/experiment-name: ''
              elyra/node-name: hello
              elyra/node-type: notebook-script
              elyra/pipeline-name: my
              elyra/pipeline-version: ''
              pipelines.kubeflow.org/cache_enabled: 'true'
          spec: null
          stepTemplate:
            computeResources: {}
            volumeMounts:
              - mountPath: /tmp
                name: mlpipeline-metrics
          steps:
            - args:
                - >
                  sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"

                  sh -c "echo 'Downloading
                  file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail
                  -H 'Cache-Control: no-cache' -L
                  file:///opt/app-root/bin/utils/bootstrapper.py --output
                  bootstrapper.py"

                  sh -c "echo 'Downloading
                  file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl
                  --fail -H 'Cache-Control: no-cache' -L
                  file:///opt/app-root/bin/utils/requirements-elyra.txt --output
                  requirements-elyra.txt"

                  sh -c "python3 -m pip install  packaging && python3 -m pip
                  freeze > requirements-current.txt && python3 bootstrapper.py
                  --pipeline-name 'my' --cos-endpoint
                  'http://minio-service:9000' --cos-bucket
                  'thy-workshop-pipelines' --cos-directory 'my-0322142832'
                  --cos-dependencies-archive
                  'hello-2654daf1-dcd9-40a4-b386-9a853cd92c10.tar.gz' --file
                  'hello.ipynb' "
              command:
                - sh
                - '-c'
              computeResources:
                limits:
                  nvidia.com/gpu: '1'
                requests:
                  cpu: '1'
                  memory: 1G
              env: # Bu listeye inject edilecek secretlar eklenecek
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: AWS_ACCESS_KEY_ID
                      name: aws-connection-pipelines
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: AWS_SECRET_ACCESS_KEY
                      name: aws-connection-pipelines
                - name: ELYRA_RUNTIME_ENV
                  value: kfp
                - name: ELYRA_ENABLE_PIPELINE_INFO
                  value: 'True'
                - name: ELYRA_WRITABLE_CONTAINER_DIR
                  value: /tmp
                - name: ELYRA_RUN_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''pipelines.kubeflow.org/run_name'']'
              image: >-
                quay.io/modh/runtime-images@sha256:b0045dc104587a6c6d5689db83c2fd4f4eac5c863969eafd7fae418316b69df1
              imagePullPolicy: IfNotPresent
              name: main
            - args:
                - >
                  #!/usr/bin/env sh

                  push_artifact() {
                      workspace_dir=$(echo $(context.taskRun.name) | sed -e "s/$(context.pipeline.name)-//g")
                      workspace_dest=/workspace/${workspace_dir}/artifacts/$(context.pipelineRun.name)/$(context.taskRun.name)
                      artifact_name=$(basename $2)

                      aws_cp() {

                        aws s3 --endpoint http://minio-service.cansutest.svc:9000 --ca-bundle /dsp-custom-certs/dsp-ca.crt cp $1.tgz s3://thy-workshop-pipelines/artifacts/$PIPELINERUN/$PIPELINETASK/$1.tgz

                      }

                      if [ -f "$workspace_dest/$artifact_name" ]; then
                          echo sending to: ${workspace_dest}/${artifact_name}
                          tar -cvzf $1.tgz -C ${workspace_dest} ${artifact_name}
                          aws_cp $1
                      elif [ -f "$2" ]; then
                          tar -cvzf $1.tgz -C $(dirname $2) ${artifact_name}
                          aws_cp $1
                      else
                          echo "$2 file does not exist. Skip artifact tracking for $1"
                      fi
                  }

                  push_log() {
                      cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log
                      push_artifact main-log step-main.log
                  }

                  strip_eof() {
                      if [ -f "$2" ]; then
                          awk 'NF' $2 | head -c -1 > $1_temp_save && cp $1_temp_save $2
                      fi
                  }

                  push_artifact "mlpipeline-metrics"
                  "/tmp/mlpipeline-metrics.json"

                  push_artifact "mlpipeline-ui-metadata"
                  "/tmp/mlpipeline-ui-metadata.json"

                  strip_eof mlpipeline-metrics /tmp/mlpipeline-metrics.json

                  strip_eof mlpipeline-ui-metadata
                  /tmp/mlpipeline-ui-metadata.json
              command:
                - sh
                - '-c'
              computeResources: {}
              env:
                - name: ARTIFACT_BUCKET
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_bucket'']'
                - name: ARTIFACT_ENDPOINT
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_endpoint'']'
                - name: ARTIFACT_ENDPOINT_SCHEME
                  valueFrom:
                    fieldRef:
                      fieldPath: >-
                        metadata.annotations['tekton.dev/artifact_endpoint_scheme']
                - name: ARTIFACT_ITEMS
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_items'']'
                - name: PIPELINETASK
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineTask'']'
                - name: PIPELINERUN
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineRun'']'
                - name: PODNAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: accesskey
                      name: mlpipeline-minio-artifact
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: secretkey
                      name: mlpipeline-minio-artifact
                - name: ARCHIVE_LOGS
                  value: 'false'
                - name: TRACK_ARTIFACTS
                  value: 'true'
                - name: STRIP_EOF
                  value: 'true'
              image: >-
                registry.redhat.io/rhoai/odh-ml-pipelines-artifact-manager-rhel8@sha256:a2bbccc967113670b7b5f62c4b60c583a9cedd0750fde1e14a06e271aa987ede
              name: copy-artifacts
              volumeMounts:
                - mountPath: /dsp-custom-certs/dsp-ca.crt
                  name: custom-ca-bundle
                  subPath: dsp-ca.crt
          volumes:
            - emptyDir: {}
              name: mlpipeline-metrics
            - configMap:
                name: dsp-trusted-ca-pipelines-definition
              name: custom-ca-bundle
      - name: run-a-file-2
        runAfter:
          - run-a-file
        taskSpec:
          metadata:
            annotations:
              elyra/node-file-name: bye.ipynb
              elyra/pipeline-source: my.pipeline
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Run a file", "outputs": [], "version": "Run a
                file@sha256=26209bdfdf1daf0a1858d75046b349209ca26ba5dc9bec949b4e084c18c0e362"}
              pipelines.kubeflow.org/task_display_name: bye
            labels:
              elyra/experiment-name: ''
              elyra/node-name: bye
              elyra/node-type: notebook-script
              elyra/pipeline-name: my
              elyra/pipeline-version: ''
              pipelines.kubeflow.org/cache_enabled: 'true'
          spec: null
          stepTemplate:
            computeResources: {}
            volumeMounts:
              - mountPath: /tmp
                name: mlpipeline-metrics
          steps:
            - args:
                - >
                  sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"

                  sh -c "echo 'Downloading
                  file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail
                  -H 'Cache-Control: no-cache' -L
                  file:///opt/app-root/bin/utils/bootstrapper.py --output
                  bootstrapper.py"

                  sh -c "echo 'Downloading
                  file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl
                  --fail -H 'Cache-Control: no-cache' -L
                  file:///opt/app-root/bin/utils/requirements-elyra.txt --output
                  requirements-elyra.txt"

                  sh -c "python3 -m pip install  packaging && python3 -m pip
                  freeze > requirements-current.txt && python3 bootstrapper.py
                  --pipeline-name 'my' --cos-endpoint
                  'http://minio-service:9000' --cos-bucket
                  'thy-workshop-pipelines' --cos-directory 'my-0322142832'
                  --cos-dependencies-archive
                  'bye-3f07d2c4-8a5a-43f3-b91c-a87fa3c85745.tar.gz' --file
                  'bye.ipynb' "
              command:
                - sh
                - '-c'
              computeResources: {}
              env:
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: AWS_ACCESS_KEY_ID
                      name: aws-connection-pipelines
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: AWS_SECRET_ACCESS_KEY
                      name: aws-connection-pipelines
                - name: ELYRA_RUNTIME_ENV
                  value: kfp
                - name: ELYRA_ENABLE_PIPELINE_INFO
                  value: 'True'
                - name: ELYRA_WRITABLE_CONTAINER_DIR
                  value: /tmp
                - name: ELYRA_RUN_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''pipelines.kubeflow.org/run_name'']'
              envFrom:
                - secretRef:
                    name: aws-connection-pipelines
                - secretRef:
                    name: aws-connection-thy-workshop
                - secretRef:
                    name: cansunewsecret
                - secretRef:
                    name: secret-fbblzj
                - secretRef:
                    name: secret-nrxbwe
                - secretRef:
                    name: aws-connection-pipelines
                - secretRef:
                    name: aws-connection-thy-workshop
                - secretRef:
                    name: cansunewsecret
                - secretRef:
                    name: secret-fbblzj
                - secretRef:
                    name: secret-nrxbwe
                - secretRef:
                    name: aws-connection-pipelines
                - secretRef:
                    name: aws-connection-thy-workshop
                - secretRef:
                    name: cansunewsecret
                - secretRef:
                    name: secret-fbblzj
                - secretRef:
                    name: secret-nrxbwe
                - secretRef:
                    name: aws-connection-pipelines
                - secretRef:
                    name: aws-connection-thy-workshop
                - secretRef:
                    name: cansunewsecret
                - secretRef:
                    name: secret-fbblzj
                - secretRef:
                    name: secret-nrxbwe
              image: >-
                quay.io/modh/runtime-images@sha256:b0045dc104587a6c6d5689db83c2fd4f4eac5c863969eafd7fae418316b69df1
              imagePullPolicy: IfNotPresent
              name: main
            - args:
                - >
                  #!/usr/bin/env sh

                  push_artifact() {
                      workspace_dir=$(echo $(context.taskRun.name) | sed -e "s/$(context.pipeline.name)-//g")
                      workspace_dest=/workspace/${workspace_dir}/artifacts/$(context.pipelineRun.name)/$(context.taskRun.name)
                      artifact_name=$(basename $2)

                      aws_cp() {

                        aws s3 --endpoint http://minio-service.cansutest.svc:9000 --ca-bundle /dsp-custom-certs/dsp-ca.crt cp $1.tgz s3://thy-workshop-pipelines/artifacts/$PIPELINERUN/$PIPELINETASK/$1.tgz

                      }

                      if [ -f "$workspace_dest/$artifact_name" ]; then
                          echo sending to: ${workspace_dest}/${artifact_name}
                          tar -cvzf $1.tgz -C ${workspace_dest} ${artifact_name}
                          aws_cp $1
                      elif [ -f "$2" ]; then
                          tar -cvzf $1.tgz -C $(dirname $2) ${artifact_name}
                          aws_cp $1
                      else
                          echo "$2 file does not exist. Skip artifact tracking for $1"
                      fi
                  }

                  push_log() {
                      cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log
                      push_artifact main-log step-main.log
                  }

                  strip_eof() {
                      if [ -f "$2" ]; then
                          awk 'NF' $2 | head -c -1 > $1_temp_save && cp $1_temp_save $2
                      fi
                  }

                  push_artifact "mlpipeline-metrics"
                  "/tmp/mlpipeline-metrics.json"

                  push_artifact "mlpipeline-ui-metadata"
                  "/tmp/mlpipeline-ui-metadata.json"

                  strip_eof mlpipeline-metrics /tmp/mlpipeline-metrics.json

                  strip_eof mlpipeline-ui-metadata
                  /tmp/mlpipeline-ui-metadata.json
              command:
                - sh
                - '-c'
              computeResources: {}
              env:
                - name: ARTIFACT_BUCKET
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_bucket'']'
                - name: ARTIFACT_ENDPOINT
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_endpoint'']'
                - name: ARTIFACT_ENDPOINT_SCHEME
                  valueFrom:
                    fieldRef:
                      fieldPath: >-
                        metadata.annotations['tekton.dev/artifact_endpoint_scheme']
                - name: ARTIFACT_ITEMS
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_items'']'
                - name: PIPELINETASK
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineTask'']'
                - name: PIPELINERUN
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineRun'']'
                - name: PODNAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: accesskey
                      name: mlpipeline-minio-artifact
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: secretkey
                      name: mlpipeline-minio-artifact
                - name: ARCHIVE_LOGS
                  value: 'false'
                - name: TRACK_ARTIFACTS
                  value: 'true'
                - name: STRIP_EOF
                  value: 'true'
              image: >-
                registry.redhat.io/rhoai/odh-ml-pipelines-artifact-manager-rhel8@sha256:a2bbccc967113670b7b5f62c4b60c583a9cedd0750fde1e14a06e271aa987ede
              name: copy-artifacts
              volumeMounts:
                - mountPath: /dsp-custom-certs/dsp-ca.crt
                  name: custom-ca-bundle
                  subPath: dsp-ca.crt
          volumes:
            - emptyDir: {}
              name: mlpipeline-metrics
            - configMap:
                name: dsp-trusted-ca-pipelines-definition
              name: custom-ca-bundle
  taskRunTemplate:
    serviceAccountName: pipeline-runner-pipelines-definition
  timeouts:
    pipeline: 1h0m0s